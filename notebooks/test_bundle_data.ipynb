{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745badd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1) DataFrame head ===\n",
      "           lfp1      lfp2\n",
      "0.000 -0.027190  0.912882\n",
      "0.002  0.094207  0.957687\n",
      "0.004  0.109566  1.029285\n",
      "0.006  0.283764  0.950148\n",
      "0.008  0.402577  0.510376 \n",
      "\n",
      "Index dtype: float64, length: 1000\n",
      "\n",
      "=== 2) RecordingBundle.signals keys & metadata ===\n",
      "  • 'lfp1': data.shape=(1000,), sampling_rate_hz=500.0, start_s=0.000\n",
      "  • 'lfp2': data.shape=(1000,), sampling_rate_hz=500.0, start_s=0.000\n",
      "\n",
      "=== 3) RecordingBundle summary ===\n",
      "  • Number of neurons: 2\n",
      "  • First 5 spike times (neuron 0): [0.08760753 0.12763451 0.1883547  0.25622727 0.30857898]\n",
      "  • First 5 spike times (neuron 1): [0.25984301 0.27950497 0.37894272 0.4538187  0.57665621]\n",
      "  • Continuous signals: ['lfp1', 'lfp2']\n",
      "\n",
      "=== 4) DecoderBatch contents ===\n",
      "  • n_time bins: 201\n",
      "  • bin_edges_s (first 5): [0.   0.01 0.02 0.03 0.04]\n",
      "  • 'counts' array shape: (201, 2)  (neurons = columns = 2)\n",
      "  • 'lfp1' shape: (201,)\n",
      "  • 'lfp2' shape: (201,)\n",
      "\n",
      "  Counts (first 10 bins):\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]] \n",
      "\n",
      "=== 5a) Sliced DecoderBatch (bins 50:100) ===\n",
      "  • New n_time = 50  (should be 50)\n",
      "  • Sliced counts shape: (50, 2)\n",
      "  • Sliced lfp1 shape: (50,)\n",
      "\n",
      "=== 5b) select_signals(['lfp2']) on sliced ===\n",
      "  • Keys now: ['lfp2']\n",
      "  • lfp2 shape: (50,)\n",
      "\n",
      "=== 5c) select_spikes([1]) ===\n",
      "  • Remaining neurons: 1\n",
      "  • First 5 spike times (neuron 1): [0.25984301 0.27950497 0.37894272 0.4538187  0.57665621]\n",
      "\n",
      "=== 6) validate_sources error ===\n",
      "DecoderBatch missing required fields:\n",
      "  • calcium\n",
      "=== 6) validate_sources success for ['counts','lfp1'] ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "demo_workflow.py\n",
    "\n",
    "A short script illustrating how to:\n",
    "  1) Create simulated continuous‐rate signals (DataFrame)\n",
    "  2) Convert them into a RecordingBundle via df_to_recording_bundle\n",
    "  3) Add synthetic spike‐train arrays to the RecordingBundle\n",
    "  4) Run to_decoder_batch to produce a time‐aligned DecoderBatch\n",
    "  5) Slice and select parts of the DecoderBatch\n",
    "  6) Validate that required sources exist\n",
    "\n",
    "Requires:\n",
    "    numpy, pandas, sklearn (for preprocessing), and the non_local_detector package\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from non_local_detector.bundle import RecordingBundle, DecoderBatch, validate_sources\n",
    "from non_local_detector.io.preprocessing import df_to_recording_bundle, to_decoder_batch\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Simulate continuous‐rate signals as a pandas DataFrame\n",
    "# ------------------------------------------------------------\n",
    "# Suppose we have two analog channels (\"lfp1\", \"lfp2\") sampled uniformly at 500 Hz\n",
    "fs = 500.0  # Hz\n",
    "n_seconds = 2.0\n",
    "n_samples = int(n_seconds * fs)\n",
    "time_index = np.linspace(0, n_seconds, n_samples, endpoint=False)\n",
    "\n",
    "# Create two sine‐wave signals (plus a bit of noise)\n",
    "lfp1 = np.sin(2 * np.pi * 10 * time_index) + 0.1 * np.random.randn(n_samples)\n",
    "lfp2 = np.cos(2 * np.pi * 20 * time_index) + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"lfp1\": lfp1,\n",
    "        \"lfp2\": lfp2,\n",
    "    },\n",
    "    index=time_index.astype(float),\n",
    ")\n",
    "\n",
    "print(\"=== 1) DataFrame head ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(f\"Index dtype: {df.index.dtype}, length: {len(df)}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Convert DataFrame → RecordingBundle.signals\n",
    "# ------------------------------------------------------------\n",
    "# We rely on df_to_recording_bundle to infer the 500 Hz sampling rate automatically.\n",
    "rb_cont = df_to_recording_bundle(df)\n",
    "print(\"=== 2) RecordingBundle.signals keys & metadata ===\")\n",
    "for key, ts in rb_cont.signals.items():\n",
    "    print(\n",
    "        f\"  • {key!r}: data.shape={ts.data.shape}, sampling_rate_hz={ts.sampling_rate_hz:.1f}, start_s={ts.start_s:.3f}\"\n",
    "    )\n",
    "print()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Add synthetic spike trains (two “neurons”)\n",
    "# ------------------------------------------------------------\n",
    "# Create two Poisson‐like spike trains over the same 2‐second interval\n",
    "# Here we just randomly generate spike times in [0, 2).\n",
    "rng = np.random.default_rng(seed=42)\n",
    "n_spikes_neuron0 = 30\n",
    "n_spikes_neuron1 = 20\n",
    "\n",
    "spike_times_0 = np.sort(rng.uniform(0, n_seconds, size=n_spikes_neuron0))\n",
    "spike_times_1 = np.sort(rng.uniform(0, n_seconds, size=n_spikes_neuron1))\n",
    "\n",
    "# The RecordingBundle API expects a list of numpy arrays for spike_times_s\n",
    "rb = RecordingBundle(\n",
    "    spike_times_s=[spike_times_0, spike_times_1],\n",
    "    spike_waveforms=None,  # no waveforms for this demo\n",
    "    signals=rb_cont.signals,  # reuse the continuous signals from step (2)\n",
    ")\n",
    "print(\"=== 3) RecordingBundle summary ===\")\n",
    "print(f\"  • Number of neurons: {len(rb.spike_times_s)}\")\n",
    "print(f\"  • First 5 spike times (neuron 0): {rb.spike_times_s[0][:5]}\")\n",
    "print(f\"  • First 5 spike times (neuron 1): {rb.spike_times_s[1][:5]}\")\n",
    "print(f\"  • Continuous signals: {list(rb.signals.keys())}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Build a DecoderBatch (bin_width = 0.01 s = 10 ms)\n",
    "# ------------------------------------------------------------\n",
    "bin_width_s = 0.01\n",
    "batch = to_decoder_batch(\n",
    "    rb,\n",
    "    bin_width_s=bin_width_s,\n",
    "    signals_to_use=[\"lfp1\", \"lfp2\", \"counts\"],  # “counts” tells it to bin spike_times_s\n",
    "    count_method=\"hist\",  # default histogram\n",
    "    float_downsample=\"mean\",\n",
    "    float_fill=\"ffill\",\n",
    "    int_fill=\"pad_zero\",\n",
    "    bool_fill=\"or\",\n",
    "    nan_policy=\"warn\",\n",
    "    one_hot_categories=False,  # not relevant since no categorical streams here\n",
    ")\n",
    "print(\"=== 4) DecoderBatch contents ===\")\n",
    "print(f\"  • n_time bins: {batch.n_time}\")\n",
    "print(f\"  • bin_edges_s (first 5): {batch.bin_edges_s[:5]}\")\n",
    "print(\n",
    "    f\"  • 'counts' array shape: {batch.counts.shape}  (neurons = columns = {batch.counts.shape[1]})\"\n",
    ")\n",
    "print(f\"  • 'lfp1' shape: {batch.signals['lfp1'].shape}\")\n",
    "print(f\"  • 'lfp2' shape: {batch.signals['lfp2'].shape}\\n\")\n",
    "\n",
    "# Show a few time bins and their spike counts\n",
    "print(\"  Counts (first 10 bins):\")\n",
    "print(batch.counts[:10, :], \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Slice and select parts of the DecoderBatch\n",
    "# ------------------------------------------------------------\n",
    "# a) Slice bins 50 through 100 (i.e., 0.50s → 1.00s)\n",
    "start_bin = 50\n",
    "stop_bin = 100\n",
    "sliced = batch.slice(start_bin, stop_bin, slice_spikes=True)\n",
    "print(f\"=== 5a) Sliced DecoderBatch (bins {start_bin}:{stop_bin}) ===\")\n",
    "print(f\"  • New n_time = {sliced.n_time}  (should be {stop_bin - start_bin})\")\n",
    "print(f\"  • Sliced counts shape: {sliced.counts.shape}\")\n",
    "print(f\"  • Sliced lfp1 shape: {sliced.signals['lfp1'].shape}\\n\")\n",
    "\n",
    "# b) Select only “lfp2” from the sliced batch\n",
    "sub_signals = sliced.select_signals([\"lfp2\"])\n",
    "print(\"=== 5b) select_signals(['lfp2']) on sliced ===\")\n",
    "print(f\"  • Keys now: {list(sub_signals.signals.keys())}\")\n",
    "print(f\"  • lfp2 shape: {sub_signals.signals['lfp2'].shape}\\n\")\n",
    "\n",
    "# c) Select only neuron #1’s spikes (from the original batch)\n",
    "selected_spikes = batch.select_spikes([1])\n",
    "print(\"=== 5c) select_spikes([1]) ===\")\n",
    "print(f\"  • Remaining neurons: {len(selected_spikes.spike_times_s)}\")\n",
    "print(f\"  • First 5 spike times (neuron 1): {selected_spikes.spike_times_s[0][:5]}\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Validate that a hypothetical model’s required sources are present\n",
    "# ------------------------------------------------------------\n",
    "class ExampleModel:\n",
    "    required_sources = [\"counts\", \"lfp1\", \"lfp2\", \"calcium\"]\n",
    "\n",
    "\n",
    "try:\n",
    "    validate_sources(batch, [ExampleModel()])\n",
    "except ValueError as e:\n",
    "    print(\"=== 6) validate_sources error ===\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# If we only require \"counts\" and \"lfp1\", it should pass\n",
    "class AnotherModel:\n",
    "    required_sources = [\"counts\", \"lfp1\"]\n",
    "\n",
    "\n",
    "validate_sources(batch, [AnotherModel()])\n",
    "print(\"=== 6) validate_sources success for ['counts','lfp1'] ===\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Done\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5c8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_local_detector2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
