{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from non_local_detector.core import filter, smoother\n",
    "\n",
    "n_time = 10\n",
    "n_states = 2\n",
    "initial_distribution = np.ones(n_states) / n_states\n",
    "# transition_matrix = np.ones((n_states, n_states)) / n_states\n",
    "transition_matrix = np.array([[0.9, 0.1], [0.1, 0.9]])\n",
    "log_likelihood = np.random.rand(n_time, n_states)\n",
    "(log_normalizer, predicted_probs_next), (filtered_probs, predicted_probs) = filter(\n",
    "    initial_distribution, transition_matrix, log_likelihood\n",
    ")\n",
    "smoothed_probs_next, smoothed_probs = smoother(transition_matrix, filtered_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk logic.\n",
    "\n",
    "1. Figure out length of chunk and index\n",
    "2. Compute log likelihood of chunk data\n",
    "3. Run forward filter\n",
    "4. Get next chunk\n",
    "5. Compute log likelihood of chunk data\n",
    "6. Run forward filter\n",
    "7. Run smoother\n",
    "8. Get next chunk\n",
    "9. Run smoother\n",
    "10. Get previous chunk\n",
    "11. Run smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0 1 2 3 4]\n",
      "1 [5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chunks = 2\n",
    "\n",
    "marginal_likelihood2 = 0.0\n",
    "filtered_probs2 = []\n",
    "predicted_probs2 = []\n",
    "\n",
    "for chunk_id, time_inds in enumerate(np.array_split(np.arange(n_time), n_chunks)):\n",
    "    print(chunk_id, time_inds)\n",
    "    if chunk_id == 0:\n",
    "        initial = initial_distribution\n",
    "    else:\n",
    "        initial = predicted_probs_next\n",
    "    chunk_log_likelihood = log_likelihood[time_inds]\n",
    "    (marginal_likelihood_chunk, predicted_probs_next), (\n",
    "        filtered_probs_chunk,\n",
    "        predicted_probs_chunk,\n",
    "    ) = filter(initial, transition_matrix, chunk_log_likelihood)\n",
    "    filtered_probs2.append(filtered_probs_chunk)\n",
    "    predicted_probs2.append(predicted_probs_chunk)\n",
    "    marginal_likelihood2 += marginal_likelihood_chunk\n",
    "\n",
    "filtered_probs2 = np.concatenate(filtered_probs2)\n",
    "predicted_probs2 = np.concatenate(predicted_probs2)\n",
    "\n",
    "np.allclose(log_normalizer, marginal_likelihood2), np.allclose(\n",
    "    filtered_probs, filtered_probs2\n",
    "), np.allclose(predicted_probs, predicted_probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [5 6 7 8 9]\n",
      "1 [0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_probs2 = []\n",
    "\n",
    "for chunk_id, time_inds in enumerate(np.array_split(np.arange(n_time), n_chunks)[::-1]):\n",
    "    print(chunk_id, time_inds)\n",
    "    if chunk_id == 0:\n",
    "        initial = filtered_probs[-1]\n",
    "    else:\n",
    "        initial = smoothed_probs_chunk[0]\n",
    "    _, smoothed_probs_chunk = smoother(\n",
    "        transition_matrix,\n",
    "        filtered_probs[time_inds],\n",
    "        initial=initial,\n",
    "        ind=time_inds,\n",
    "        n_time=n_time,\n",
    "    )\n",
    "    smoothed_probs2.append(smoothed_probs_chunk)\n",
    "\n",
    "smoothed_probs3 = np.concatenate(smoothed_probs2[::-1])\n",
    "\n",
    "np.allclose(smoothed_probs, smoothed_probs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things needed to estimate parameters:\n",
    "- acausal_posterior\n",
    "- causal_state_probabilities\n",
    "- predictive_state_probabilities\n",
    "- acausal_state_probabilities\n",
    "\n",
    "Things that can be discarded:\n",
    "- log_likelihood\n",
    "- causal_posterior\n",
    "\n",
    "\n",
    "Psuedo code:\n",
    "```python\n",
    "\n",
    "filtered_probs = []\n",
    "predictive_state_probabilities = []\n",
    "causal_state_probabilities = []\n",
    "marginal_likelihood = 0.0\n",
    "\n",
    "time_chunks = np.array_split(np.arange(n_time), n_chunks)\n",
    "\n",
    "for chunk_id, time_inds in enumerate(time_chunks):\n",
    "    if is_missing is not None:\n",
    "        is_missing_chunk = is_missing[time_inds]\n",
    "    log_likelihood_chunk = compute_log_likelihood(\n",
    "        time[time_inds],\n",
    "        position_time,\n",
    "        position,\n",
    "        spike_times,\n",
    "        is_missing=is_missing_chunk,\n",
    "    )\n",
    "    initial = initial_distribution if chunk_id == 0 else predicted_probs_next\n",
    "    \n",
    "    (marginal_likelihood_chunk, predicted_probs_next), (\n",
    "        filtered_probs_chunk,\n",
    "        predicted_probs_chunk,\n",
    "    ) = filter(initial, transition_matrix, chunk_log_likelihood)\n",
    "\n",
    "    filtered_probs.append(filtered_probs_chunk)\n",
    "    \n",
    "    causal_state_probabilities.append(convert_to_state_probability(filtered_probs_chunk))\n",
    "    predictive_state_probabilities.append(convert_to_state_probability(predicted_probs_chunk))\n",
    "\n",
    "    marginal_likelihood += marginal_likelihood_chunk\n",
    "\n",
    "smoothed_probs = []\n",
    "acausal_state_probabilities = []\n",
    "\n",
    "for chunk_id, time_inds in enumerate(time_chunks[::-1]):\n",
    "    initial = filtered_probs[-1] if chunk_id == 0 else smoothed_probs_chunk[0]\n",
    "    _, smoothed_probs_chunk = smoother(\n",
    "        transition_matrix,\n",
    "        filtered_probs[time_inds],\n",
    "        initial=initial,\n",
    "        ind=time_inds,\n",
    "        n_time=n_time,\n",
    "    )\n",
    "    smoothed_probs.append(smoothed_probs_chunk)\n",
    "    acausal_state_probabilities.append(convert_to_state_probability(smoothed_probs_chunk))\n",
    "\n",
    "\n",
    "return (\n",
    "    smoothed_probs,\n",
    "    acausal_state_probabilities,\n",
    "    causal_state_probabilities,\n",
    "    predictive_state_probabilities,\n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(time, data, is_missing=None):\n",
    "    return data[time]\n",
    "\n",
    "\n",
    "def chunked_filter_smoother(\n",
    "    time,\n",
    "    data,\n",
    "    state_ind,\n",
    "    initial_distribution,\n",
    "    transition_matrix,\n",
    "    is_missing=None,\n",
    "    n_chunks=1,\n",
    "):\n",
    "    causal_posterior = []\n",
    "    predictive_state_probabilities = []\n",
    "    causal_state_probabilities = []\n",
    "    acausal_posterior = []\n",
    "    acausal_state_probabilities = []\n",
    "    marginal_likelihood = 0.0\n",
    "\n",
    "    n_time = len(time)\n",
    "    time_chunks = np.array_split(np.arange(n_time), n_chunks)\n",
    "\n",
    "    n_states = len(np.unique(state_ind))\n",
    "    state_mask = np.identity(n_states)[state_ind]  # shape (n_state_inds, n_states)\n",
    "\n",
    "    for chunk_id, time_inds in enumerate(time_chunks):\n",
    "        is_missing_chunk = is_missing[time_inds] if is_missing is not None else None\n",
    "        log_likelihood_chunk = compute_log_likelihood(\n",
    "            time[time_inds],\n",
    "            data,\n",
    "            is_missing=is_missing_chunk,\n",
    "        )\n",
    "\n",
    "        (marginal_likelihood_chunk, predicted_probs_next), (\n",
    "            causal_posterior_chunk,\n",
    "            predicted_probs_chunk,\n",
    "        ) = filter(\n",
    "            initial_distribution=(\n",
    "                initial_distribution if chunk_id == 0 else predicted_probs_next\n",
    "            ),\n",
    "            transition_matrix=transition_matrix,\n",
    "            log_likelihoods=log_likelihood_chunk,\n",
    "        )\n",
    "\n",
    "        causal_posterior_chunk = np.asarray(causal_posterior_chunk)\n",
    "        causal_posterior.append(causal_posterior_chunk)\n",
    "        causal_state_probabilities.append(causal_posterior_chunk @ state_mask)\n",
    "        predictive_state_probabilities.append(predicted_probs_chunk @ state_mask)\n",
    "\n",
    "        marginal_likelihood += marginal_likelihood_chunk\n",
    "\n",
    "    causal_posterior = np.concatenate(causal_posterior)\n",
    "\n",
    "    for chunk_id, time_inds in enumerate(reversed(time_chunks)):\n",
    "        _, acausal_posterior_chunk = smoother(\n",
    "            transition_matrix=transition_matrix,\n",
    "            filtered_probs=causal_posterior[time_inds],\n",
    "            initial=(\n",
    "                causal_posterior[-1] if chunk_id == 0 else acausal_posterior_chunk[0]\n",
    "            ),\n",
    "            ind=time_inds,\n",
    "            n_time=n_time,\n",
    "        )\n",
    "        acausal_posterior_chunk = np.asarray(acausal_posterior_chunk)\n",
    "        acausal_posterior.append(acausal_posterior_chunk)\n",
    "        acausal_state_probabilities.append(acausal_posterior_chunk @ state_mask)\n",
    "\n",
    "    acausal_posterior = np.concatenate(acausal_posterior[::-1])\n",
    "    acausal_state_probabilities = np.concatenate(acausal_state_probabilities[::-1])\n",
    "\n",
    "    return (\n",
    "        acausal_posterior,\n",
    "        acausal_state_probabilities,\n",
    "        causal_state_probabilities,\n",
    "        predictive_state_probabilities,\n",
    "        marginal_likelihood,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    acausal_posterior,\n",
    "    acausal_state_probabilities,\n",
    "    causal_state_probabilities,\n",
    "    predictive_state_probabilities,\n",
    "    marginal_likelihood,\n",
    ") = chunked_filter_smoother(\n",
    "    time=np.arange(n_time),\n",
    "    data=log_likelihood,\n",
    "    state_ind=np.arange(n_states),\n",
    "    initial_distribution=initial_distribution,\n",
    "    transition_matrix=transition_matrix,\n",
    "    n_chunks=1,\n",
    ")\n",
    "\n",
    "np.allclose(acausal_posterior, smoothed_probs), np.allclose(\n",
    "    marginal_likelihood, log_normalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_local_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
