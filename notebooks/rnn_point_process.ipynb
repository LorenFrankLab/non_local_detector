{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fae0c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Simulating data...\n",
      "2. Starting model training...\n",
      "Epoch 20/200, Avg. Loss: 0.8651\n",
      "Epoch 40/200, Avg. Loss: 0.8349\n",
      "Epoch 60/200, Avg. Loss: 0.8102\n",
      "Epoch 80/200, Avg. Loss: 0.7948\n",
      "Epoch 100/200, Avg. Loss: 0.7818\n",
      "Epoch 120/200, Avg. Loss: 0.7674\n",
      "Epoch 140/200, Avg. Loss: 0.7533\n",
      "Epoch 160/200, Avg. Loss: 0.7397\n",
      "Epoch 180/200, Avg. Loss: 0.7251\n",
      "Epoch 200/200, Avg. Loss: 0.7145\n",
      "3. Training complete.\n",
      "   - Saved training loss plot to 'training_loss.png'\n",
      "\n",
      "4. Gathering predictions from the test set...\n",
      "5. Generating evaluation plots...\n",
      "   - Saved prediction example plot.\n",
      "   - Saved time error distribution plot.\n",
      "   - Saved confusion matrix plot.\n",
      "   - Saved intensity function plot.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script provides a complete implementation of the Recurrent Marked Temporal\n",
    "Point Process (RMTPP) model using Flax's nnx module. It includes data\n",
    "simulation, model definition, training, and prediction.\n",
    "\"\"\"\n",
    "\n",
    "import functools\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import seaborn as sns\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Constants for data simulation and model configuration\n",
    "N_SEQS = 200  # Increased for a larger test set\n",
    "N_MARKS = 5\n",
    "MAX_SEQ_LEN = 50\n",
    "HIDDEN_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "\n",
    "\n",
    "def simulate_data(\n",
    "    n_sequences: int, max_len: int, n_marks: int\n",
    ") -> List[Dict[str, jnp.ndarray]]:\n",
    "    \"\"\"\n",
    "    Simulates marked temporal point process data.\n",
    "\n",
    "    A simple Hawkes-like process is simulated where certain event marks can\n",
    "    excite or inhibit other marks, influencing the time to the next event.\n",
    "\n",
    "    Args:\n",
    "        n_sequences: The number of sequences to generate.\n",
    "        max_len: The maximum length of any sequence.\n",
    "        n_marks: The number of unique event types (marks).\n",
    "\n",
    "    Returns:\n",
    "        A list of sequences. Each sequence is a dictionary containing the\n",
    "        event times, marks, and a mask for padding.\n",
    "    \"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    sequences = []\n",
    "\n",
    "    # Create an influence matrix: mark_i -> mark_j\n",
    "    key, subkey = random.split(key)\n",
    "    influence_matrix = random.uniform(\n",
    "        subkey, (n_marks, n_marks), minval=-0.5, maxval=1.0\n",
    "    )\n",
    "\n",
    "    for _ in range(n_sequences):\n",
    "        key, subkey_len, subkey_time, subkey_mark = random.split(key, 4)\n",
    "        seq_len = random.randint(subkey_len, (), minval=10, maxval=max_len)\n",
    "\n",
    "        times = np.zeros(max_len, dtype=np.float32)\n",
    "        marks = np.zeros(max_len, dtype=np.int32)\n",
    "\n",
    "        # Initialize first event\n",
    "        marks[0] = random.randint(subkey_mark, (), 0, n_marks)\n",
    "        times[0] = random.exponential(subkey_time, ()) * 0.1\n",
    "\n",
    "        # Generate subsequent events\n",
    "        for i in range(1, seq_len):\n",
    "            key, subkey_time, subkey_mark = random.split(key, 3)\n",
    "\n",
    "            # Intensity is influenced by the previous mark\n",
    "            last_mark = marks[i - 1]\n",
    "            base_rate = 1.0\n",
    "            # Next event's rate is influenced by all possible next marks\n",
    "            rates = base_rate + influence_matrix[last_mark, :]\n",
    "            rates = jnp.maximum(rates, 0.1)  # Ensure positive rates\n",
    "\n",
    "            # Sample time from exponential dist with the sum of rates\n",
    "            total_rate = jnp.sum(rates)\n",
    "            inter_event_time = random.exponential(subkey_time, ()) / total_rate\n",
    "            times[i] = times[i - 1] + inter_event_time\n",
    "\n",
    "            # Sample next mark based on individual rates\n",
    "            probabilities = rates / total_rate\n",
    "            marks[i] = random.choice(subkey_mark, n_marks, p=probabilities)\n",
    "\n",
    "        mask = np.arange(max_len) < seq_len\n",
    "        sequences.append(\n",
    "            {\n",
    "                \"times\": jnp.asarray(times),\n",
    "                \"marks\": jnp.asarray(marks),\n",
    "                \"mask\": jnp.asarray(mask),\n",
    "            }\n",
    "        )\n",
    "    return sequences\n",
    "\n",
    "\n",
    "class RMTPP(nnx.Module):\n",
    "    \"\"\"\n",
    "        Recurrent Marked Temporal Point Process (RMTPP) model.\n",
    "\n",
    "        This class implements the architecture described in the paper, using an RNN\n",
    "        to learn a representation of event history and predict the time and mark\n",
    "    of\n",
    "        the next event.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        n_marks: int,\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the RMTPP model layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: The dimensionality of the hidden state vector.\n",
    "            n_marks: The number of unique event marks.\n",
    "            rngs: JAX random number generators.\n",
    "        \"\"\"\n",
    "        self.embedding = nnx.Embed(\n",
    "            num_embeddings=n_marks, features=hidden_size, rngs=rngs\n",
    "        )\n",
    "        self.recurrent_update_w = nnx.Linear(\n",
    "            in_features=hidden_size, out_features=hidden_size, use_bias=False, rngs=rngs\n",
    "        )\n",
    "        self.recurrent_update_h = nnx.Linear(\n",
    "            in_features=hidden_size, out_features=hidden_size, use_bias=True, rngs=rngs\n",
    "        )\n",
    "        self.recurrent_update_t = nnx.Linear(\n",
    "            in_features=1, out_features=hidden_size, use_bias=False, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # Output layers for intensity function parameters and mark prediction\n",
    "        self.v_proj = nnx.Linear(hidden_size, 1, use_bias=False, rngs=rngs)\n",
    "        self.w_proj = nnx.Linear(hidden_size, 1, use_bias=False, rngs=rngs)\n",
    "        self.b_proj = nnx.Linear(hidden_size, 1, use_bias=True, rngs=rngs)\n",
    "        self.mark_proj = nnx.Linear(hidden_size, n_marks, use_bias=True, rngs=rngs)\n",
    "\n",
    "    def __call__(\n",
    "        self, times: jnp.ndarray, marks: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            times: A (batch, seq_len) array of event times.\n",
    "            marks: A (batch, seq_len) array of event marks.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - Predicted logits for the next mark.\n",
    "            - The 'v' parameter of the intensity function.\n",
    "            - The 'w' parameter of the intensity function.\n",
    "            - The 'b' parameter of the intensity function.\n",
    "        \"\"\"\n",
    "        # Calculate inter-event times (d_j = t_j - t_{j-1})\n",
    "        inter_event_times = jnp.diff(times, prepend=0)\n",
    "        inter_event_times = jnp.expand_dims(inter_event_times, -1)\n",
    "\n",
    "        # Scan over the sequence to update the hidden state\n",
    "        def rnn_step(h_prev, inputs):\n",
    "            mark_j, time_j = inputs\n",
    "            w_emb = self.embedding(mark_j)\n",
    "\n",
    "            # Equation (9) from the paper\n",
    "            h_j = jax.nn.relu(\n",
    "                self.recurrent_update_w(w_emb)\n",
    "                + self.recurrent_update_h(h_prev)\n",
    "                + self.recurrent_update_t(time_j)\n",
    "            )\n",
    "            return h_j, h_j\n",
    "\n",
    "        # Initialize hidden state\n",
    "        batch_size = times.shape[0]\n",
    "        h0 = jnp.zeros((batch_size, self.recurrent_update_h.in_features))\n",
    "\n",
    "        # Prepare inputs for scan: shape must be (seq_len, batch_size, ...)\n",
    "        marks_scannable = marks.T\n",
    "        inter_event_times_scannable = jnp.transpose(inter_event_times, (1, 0, 2))\n",
    "\n",
    "        # Run the RNN\n",
    "        scan_fn = nnx.scan(rnn_step)\n",
    "        _, h_sequence = scan_fn(h0, (marks_scannable, inter_event_times_scannable))\n",
    "        h_sequence = h_sequence.transpose((1, 0, 2))\n",
    "\n",
    "        # Project hidden states to get predictions\n",
    "        mark_logits = self.mark_proj(h_sequence)\n",
    "\n",
    "        # Stabilize the outputs for v and b to prevent overflow in the exp function\n",
    "        v_unconstrained = self.v_proj(h_sequence)\n",
    "        b_unconstrained = self.b_proj(h_sequence)\n",
    "        v = jax.nn.tanh(v_unconstrained) * 5.0\n",
    "        b = jax.nn.tanh(b_unconstrained) * 5.0\n",
    "\n",
    "        # Softplus ensures w > 0, and epsilon prevents division by zero.\n",
    "        w = jax.nn.softplus(self.w_proj(h_sequence)) + 1e-6\n",
    "\n",
    "        return mark_logits, v, w, b\n",
    "\n",
    "\n",
    "def compute_loss(model: RMTPP, batch: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the negative log-likelihood loss for a batch of sequences.\n",
    "\n",
    "    Args:\n",
    "        model: The RMTPP model instance.\n",
    "        batch: A dictionary containing 'times', 'marks', and 'mask'.\n",
    "\n",
    "    Returns:\n",
    "        The total loss for the batch.\n",
    "    \"\"\"\n",
    "    times, marks, mask = batch[\"times\"], batch[\"marks\"], batch[\"mask\"]\n",
    "\n",
    "    # The last valid event in each sequence has no future event to predict.\n",
    "    # We create a new mask to exclude the last event of each sequence from loss calculation.\n",
    "    is_last_event = jnp.cumsum(mask, axis=1) == jnp.sum(mask, axis=1, keepdims=True)\n",
    "    prediction_mask = mask & ~is_last_event\n",
    "\n",
    "    # Shift targets for prediction: we predict event j+1 from event j\n",
    "    targets_marks = jnp.roll(marks, -1, axis=1)\n",
    "    targets_times = jnp.roll(times, -1, axis=1)\n",
    "    inter_event_times_target = targets_times - times\n",
    "\n",
    "    mark_logits, v, w, b = model(times, marks)\n",
    "    v, w, b = v.squeeze(-1), w.squeeze(-1), b.squeeze(-1)\n",
    "\n",
    "    # 1. Mark Loss (Cross-Entropy)\n",
    "    mark_loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        mark_logits, targets_marks\n",
    "    )\n",
    "    # Apply mask to ignore loss from padded elements and the last real event\n",
    "    mark_loss = jnp.sum(mark_loss * prediction_mask) / jnp.sum(prediction_mask)\n",
    "\n",
    "    # 2. Time Loss (Negative Log-Likelihood)\n",
    "    # Using the more stable formulation: -log(lambda) + integral(lambda)\n",
    "    log_lambda = v + w * inter_event_times_target + b\n",
    "\n",
    "    integral_lambda = (1 / w) * (\n",
    "        jnp.exp(v + w * inter_event_times_target + b) - jnp.exp(v + b)\n",
    "    )\n",
    "\n",
    "    time_loss = -log_lambda + integral_lambda\n",
    "    time_loss = jnp.sum(time_loss * prediction_mask) / jnp.sum(prediction_mask)\n",
    "\n",
    "    return mark_loss + time_loss\n",
    "\n",
    "\n",
    "@functools.partial(nnx.jit, static_argnums=(0, 1))\n",
    "def train_step(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    tx: optax.GradientTransformation,\n",
    "    optimizer_state: optax.OptState,\n",
    "    params: nnx.State,\n",
    "    batch: Dict[str, jnp.ndarray],\n",
    ") -> Tuple[float, optax.OptState, nnx.State]:\n",
    "    \"\"\"\n",
    "    Performs a single training step within a JIT-compiled function.\n",
    "    This version works with a split model (GraphDef and State) for compatibility\n",
    "    with JAX transformations.\n",
    "\n",
    "    Args:\n",
    "        graphdef: The static graph definition of the model.\n",
    "        tx: The Optax optimizer.\n",
    "        optimizer_state: The current state of the optimizer.\n",
    "        params: The learnable parameters of the model.\n",
    "        batch: A batch of training data.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of the loss, updated optimizer state, and updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(current_params: nnx.State) -> float:\n",
    "        \"\"\"Calculates loss for the given parameters.\"\"\"\n",
    "        # Reconstruct the model inside the loss function\n",
    "        model = nnx.merge(graphdef, current_params)\n",
    "        return compute_loss(model, batch)\n",
    "\n",
    "    # Use jax.value_and_grad directly on the function of params\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(params)\n",
    "\n",
    "    updates, optimizer_state = tx.update(grads, optimizer_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, optimizer_state, params\n",
    "\n",
    "\n",
    "@functools.partial(nnx.jit, static_argnums=(0,))\n",
    "def predict_next_event(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    params: nnx.State,\n",
    "    history_times: jnp.ndarray,\n",
    "    history_marks: jnp.ndarray,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Predicts the time and mark of the next event given a history.\n",
    "\n",
    "    Args:\n",
    "        graphdef: The static graph definition of the model.\n",
    "        params: The trained parameters of the model.\n",
    "        history_times: A (seq_len,) array of event times in the history.\n",
    "        history_marks: A (seq_len,) array of event marks in the history.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the predicted absolute time, the predicted mark,\n",
    "        and the intensity parameters (v, w, b) for the last step.\n",
    "    \"\"\"\n",
    "    # Reconstruct the model inside the JIT'd function\n",
    "    model = nnx.merge(graphdef, params)\n",
    "\n",
    "    # Add a batch dimension for the model\n",
    "    times_batch = jnp.expand_dims(history_times, 0)\n",
    "    marks_batch = jnp.expand_dims(history_marks, 0)\n",
    "\n",
    "    # Get model outputs for the history\n",
    "    mark_logits, v, w, b = model(times_batch, marks_batch)\n",
    "\n",
    "    # Get the outputs corresponding to the *last* event in the sequence\n",
    "    last_mark_logits = mark_logits[0, -1, :]\n",
    "    last_v = v[0, -1, 0]\n",
    "    last_w = w[0, -1, 0]\n",
    "    last_b = b[0, -1, 0]\n",
    "\n",
    "    # 1. Predict next mark (the one with the highest probability)\n",
    "    predicted_mark = jnp.argmax(last_mark_logits)\n",
    "\n",
    "    # 2. Predict next time\n",
    "    # Instead of numerical integration for the mean, we compute the median\n",
    "    # which has a simple analytical form.\n",
    "    # delta_t = (1/w) * log(1 + (w * log(2)) / exp(v+b))\n",
    "    A = last_v + last_b\n",
    "    predicted_inter_event_time = (1 / last_w) * jnp.log(\n",
    "        1 + (last_w * jnp.log(2.0)) / jnp.exp(A)\n",
    "    )\n",
    "\n",
    "    last_time = history_times[-1]\n",
    "    predicted_time = last_time + predicted_inter_event_time\n",
    "\n",
    "    return predicted_time, predicted_mark, last_v, last_w, last_b\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    tx: optax.GradientTransformation,\n",
    "    params: nnx.State,\n",
    "    sequences: List[Dict[str, jnp.ndarray]],\n",
    ") -> Tuple[nnx.State, List[float]]:\n",
    "    \"\"\"\n",
    "    Handles the main training loop for the RMTPP model.\n",
    "\n",
    "    Args:\n",
    "        graphdef: The static graph definition of the model.\n",
    "        tx: The Optax optimizer.\n",
    "        params: The initial learnable parameters of the model.\n",
    "        sequences: The list of training sequences.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the trained parameters and the list of losses per\n",
    "        epoch.\n",
    "    \"\"\"\n",
    "    print(\"2. Starting model training...\")\n",
    "    optimizer_state = tx.init(params)\n",
    "    losses = []\n",
    "    n_batches = len(sequences) // BATCH_SIZE\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        # Simple batching\n",
    "        np.random.shuffle(sequences)\n",
    "        for i in range(n_batches):\n",
    "            batch_list = sequences[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "\n",
    "            # Pad sequences to be the same length for batching\n",
    "            batch = {\n",
    "                \"times\": jnp.stack([s[\"times\"] for s in batch_list]),\n",
    "                \"marks\": jnp.stack([s[\"marks\"] for s in batch_list]),\n",
    "                \"mask\": jnp.stack([s[\"mask\"] for s in batch_list]),\n",
    "            }\n",
    "\n",
    "            loss_val, optimizer_state, params = train_step(\n",
    "                graphdef, tx, optimizer_state, params, batch\n",
    "            )\n",
    "            epoch_loss += loss_val\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"3. Training complete.\")\n",
    "    return params, losses\n",
    "\n",
    "\n",
    "def evaluate_and_plot(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    params: nnx.State,\n",
    "    test_sequences: List[Dict[str, jnp.ndarray]],\n",
    "    losses: List[float],\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a comprehensive evaluation on the test set and generates result plots.\n",
    "\n",
    "    Args:\n",
    "        graphdef: The static graph definition of the model.\n",
    "        params: The trained parameters of the model.\n",
    "        test_sequences: A list of sequences for evaluation.\n",
    "        losses: A list of losses from training for plotting.\n",
    "    \"\"\"\n",
    "    # --- 1. Plot Training Loss ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Negative Log-Likelihood Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"training_loss.png\")\n",
    "    print(\"   - Saved training loss plot to 'training_loss.png'\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 2. Gather Predictions from Test Set ---\n",
    "    print(\"\\n4. Gathering predictions from the test set...\")\n",
    "    all_pred_marks, all_actual_marks = [], []\n",
    "    all_pred_times, all_actual_times = [], []\n",
    "    intensity_params_for_plot = None\n",
    "\n",
    "    for i, seq in enumerate(test_sequences):\n",
    "        actual_seq_len = int(jnp.sum(seq[\"mask\"]))\n",
    "        if actual_seq_len < 10:\n",
    "            continue\n",
    "\n",
    "        history_len = actual_seq_len // 2\n",
    "        actual_event_index = history_len\n",
    "\n",
    "        history_times = seq[\"times\"][:history_len]\n",
    "        history_marks = seq[\"marks\"][:history_len]\n",
    "\n",
    "        pred_time, pred_mark, v, w, b = predict_next_event(\n",
    "            graphdef, params, history_times, history_marks\n",
    "        )\n",
    "\n",
    "        actual_time = seq[\"times\"][actual_event_index]\n",
    "        actual_mark = seq[\"marks\"][actual_event_index]\n",
    "\n",
    "        all_pred_marks.append(pred_mark)\n",
    "        all_actual_marks.append(actual_mark)\n",
    "        all_pred_times.append(pred_time)\n",
    "        all_actual_times.append(actual_time)\n",
    "\n",
    "        # Save params from the first sequence for the intensity plot\n",
    "        if i == 0:\n",
    "            intensity_params_for_plot = {\n",
    "                \"v\": v,\n",
    "                \"w\": w,\n",
    "                \"b\": b,\n",
    "                \"actual_dt\": actual_time - history_times[-1],\n",
    "            }\n",
    "\n",
    "    # --- 3. Generate and Save Plots ---\n",
    "    print(\"5. Generating evaluation plots...\")\n",
    "\n",
    "    # Plot single prediction example\n",
    "    plot_prediction_example(\n",
    "        history_times,\n",
    "        history_marks,\n",
    "        all_actual_times[0],\n",
    "        all_actual_marks[0],\n",
    "        all_pred_times[0],\n",
    "        all_pred_marks[0],\n",
    "    )\n",
    "\n",
    "    # Plot Time Prediction Error Histogram\n",
    "    time_errors = np.array(all_pred_times) - np.array(all_actual_times)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(time_errors, kde=True)\n",
    "    plt.title(\"Distribution of Time Prediction Errors (Predicted - Actual)\")\n",
    "    plt.xlabel(\"Time Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"time_error_distribution.png\")\n",
    "    print(\"   - Saved time error distribution plot.\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    cm = confusion_matrix(all_actual_marks, all_pred_marks, labels=range(N_MARKS))\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, display_labels=[f\"Mark {i}\" for i in range(N_MARKS)]\n",
    "    )\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Mark Prediction Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    print(\"   - Saved confusion matrix plot.\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Intensity Function\n",
    "    plot_intensity_function(intensity_params_for_plot)\n",
    "\n",
    "\n",
    "def plot_prediction_example(h_times, h_marks, a_time, a_mark, p_time, p_mark):\n",
    "    \"\"\"Plots a single prediction example.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(h_times, h_marks, c=\"blue\", marker=\"o\", label=\"History Events\")\n",
    "    plt.scatter(a_time, a_mark, c=\"green\", marker=\"*\", s=200, label=\"Actual Next Event\")\n",
    "    plt.scatter(\n",
    "        p_time, p_mark, c=\"red\", marker=\"x\", s=200, label=\"Predicted Next Event\"\n",
    "    )\n",
    "    plt.yticks(range(N_MARKS), [f\"Mark {i}\" for i in range(N_MARKS)])\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Event Mark\")\n",
    "    plt.title(\"RMTPP Prediction vs. Actual Event\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.savefig(\"prediction_vs_actual.png\")\n",
    "    print(\"   - Saved prediction example plot.\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_intensity_function(params):\n",
    "    \"\"\"Plots the learned conditional intensity function for one example.\"\"\"\n",
    "    v, w, b = params[\"v\"], params[\"w\"], params[\"b\"]\n",
    "    actual_dt = params[\"actual_dt\"]\n",
    "\n",
    "    lambda_func = lambda t: jnp.exp(v + w * t + b)\n",
    "\n",
    "    max_t = float(actual_dt) * 2.0  # Plot up to twice the actual delta_t\n",
    "    t_range = np.linspace(0, max_t, 200)\n",
    "    intensity_values = [lambda_func(t) for t in t_range]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(t_range, intensity_values, label=\"Learned Intensity $\\lambda^*(t)$\")\n",
    "    plt.axvline(\n",
    "        x=actual_dt,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Actual Event Time ($\\Delta t={actual_dt:.2f}$)\",\n",
    "    )\n",
    "    plt.xlabel(\"Time since last event ($\\Delta t$)\")\n",
    "    plt.ylabel(\"Conditional Intensity $\\lambda^*(t)$\")\n",
    "    plt.title(\"Learned Conditional Intensity Function\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"intensity_function.png\")\n",
    "    print(\"   - Saved intensity function plot.\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run data simulation, model training, and plotting.\"\"\"\n",
    "    print(\"1. Simulating data...\")\n",
    "    sequences = simulate_data(N_SEQS, MAX_SEQ_LEN, N_MARKS)\n",
    "    np.random.shuffle(sequences)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    test_size = int(len(sequences) * TEST_SPLIT_RATIO)\n",
    "    train_sequences = sequences[test_size:]\n",
    "    test_sequences = sequences[:test_size]\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = RMTPP(hidden_size=HIDDEN_SIZE, n_marks=N_MARKS, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Chain the optimizer with gradient clipping to prevent exploding gradients\n",
    "    tx = optax.chain(optax.clip(1.0), optax.adam(LEARNING_RATE))\n",
    "\n",
    "    # Split the model into its static definition (graphdef) and dynamic state (params).\n",
    "    graphdef, params = nnx.split(model, nnx.Param)\n",
    "\n",
    "    # Train the model\n",
    "    trained_params, losses = train_model(graphdef, tx, params, train_sequences)\n",
    "\n",
    "    # Evaluate the model and plot results\n",
    "    evaluate_and_plot(graphdef, trained_params, test_sequences, losses)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b594315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Simulating waveform data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Non-exhaustive filters, got a non-empty remainder: \u001b[38;2;79;201;177mFlatState\u001b[0m\u001b[38;2;255;213;3m([\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n  \u001b[38;2;156;220;254m\u001b[0m\u001b[38;2;212;212;212m\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'gru_cell'\u001b[0m, \u001b[38;2;207;144;120m'rngs'\u001b[0m, \u001b[38;2;207;144;120m'default'\u001b[0m, \u001b[38;2;207;144;120m'count'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (4 B)\u001b[0m\n    \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngCount\u001b[0m,\n    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(21, dtype=uint32),\n    \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'default'\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n  \u001b[38;2;156;220;254m\u001b[0m\u001b[38;2;212;212;212m\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'gru_cell'\u001b[0m, \u001b[38;2;207;144;120m'rngs'\u001b[0m, \u001b[38;2;207;144;120m'default'\u001b[0m, \u001b[38;2;207;144;120m'key'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n    \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngKey\u001b[0m,\n    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n    [ 0 42],\n    \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'default'\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m\n\u001b[38;2;255;213;3m])\u001b[0m.\nUse `...` to match all remaining elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 571\u001b[0m\n\u001b[1;32m    567\u001b[0m     evaluate_and_plot(graphdef, trained_params, test_sequences, losses, config)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 571\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 565\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    563\u001b[0m model \u001b[38;5;241m=\u001b[39m RMTPP_Waveform(config\u001b[38;5;241m=\u001b[39mconfig, rngs\u001b[38;5;241m=\u001b[39mnnx\u001b[38;5;241m.\u001b[39mRngs(config\u001b[38;5;241m.\u001b[39mseed))\n\u001b[1;32m    564\u001b[0m tx \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mchain(optax\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m1.0\u001b[39m), optax\u001b[38;5;241m.\u001b[39madam(config\u001b[38;5;241m.\u001b[39mlearning_rate))\n\u001b[0;32m--> 565\u001b[0m graphdef, params \u001b[38;5;241m=\u001b[39m \u001b[43mnnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m trained_params, losses \u001b[38;5;241m=\u001b[39m train_model(graphdef, tx, params, train_sequences, config)\n\u001b[1;32m    567\u001b[0m evaluate_and_plot(graphdef, trained_params, test_sequences, losses, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/non_local_detector/lib/python3.10/site-packages/flax/nnx/graph.py:2276\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(node, *filters)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Split a graph node into a :class:`GraphDef` and one or more :class:`State`s. State is\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;124;03ma ``Mapping`` from strings or integers to ``Variables``, Arrays or nested States. GraphDef\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;124;03mcontains all the static information needed to reconstruct a ``Module`` graph, it is analogous\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;124;03m  filters are passed, a single ``State`` is returned.\u001b[39;00m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m graphdef, flat_state \u001b[38;5;241m=\u001b[39m flatten(node)\n\u001b[0;32m-> 2276\u001b[0m flat_states \u001b[38;5;241m=\u001b[39m \u001b[43m_split_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2277\u001b[0m states \u001b[38;5;241m=\u001b[39m _to_nested_state(graphdef, flat_states)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graphdef, \u001b[38;5;241m*\u001b[39mstates\n",
      "File \u001b[0;32m~/miniconda3/envs/non_local_detector/lib/python3.10/site-packages/flax/nnx/graph.py:2173\u001b[0m, in \u001b[0;36m_split_state\u001b[0;34m(state, filters)\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filters:\n\u001b[1;32m   2172\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (state,)  \u001b[38;5;66;03m# type: ignore[bad-return-type]\u001b[39;00m\n\u001b[0;32m-> 2173\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(states, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   2175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (states,)  \u001b[38;5;66;03m# type: ignore[bad-return-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/non_local_detector/lib/python3.10/site-packages/flax/nnx/statelib.py:142\u001b[0m, in \u001b[0;36mFlatState.split\u001b[0;34m(self, first, *filters)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;241m*\u001b[39mflat_states_, rest \u001b[38;5;241m=\u001b[39m _split_state(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mfilters)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rest:\n\u001b[0;32m--> 142\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-exhaustive filters, got a non-empty remainder: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUse `...` to match all remaining elements.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    145\u001b[0m   )\n\u001b[1;32m    147\u001b[0m flat_states: FlatState[V] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[FlatState[V], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flat_states_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Non-exhaustive filters, got a non-empty remainder: \u001b[38;2;79;201;177mFlatState\u001b[0m\u001b[38;2;255;213;3m([\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n  \u001b[38;2;156;220;254m\u001b[0m\u001b[38;2;212;212;212m\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'gru_cell'\u001b[0m, \u001b[38;2;207;144;120m'rngs'\u001b[0m, \u001b[38;2;207;144;120m'default'\u001b[0m, \u001b[38;2;207;144;120m'count'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (4 B)\u001b[0m\n    \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngCount\u001b[0m,\n    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(21, dtype=uint32),\n    \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'default'\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m,\n  \u001b[38;2;156;220;254m\u001b[0m\u001b[38;2;212;212;212m\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;207;144;120m'gru_cell'\u001b[0m, \u001b[38;2;207;144;120m'rngs'\u001b[0m, \u001b[38;2;207;144;120m'default'\u001b[0m, \u001b[38;2;207;144;120m'key'\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m, \u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n    \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mRngKey\u001b[0m,\n    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n    [ 0 42],\n    \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'default'\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\u001b[38;2;255;213;3m)\u001b[0m\n\u001b[38;2;255;213;3m])\u001b[0m.\nUse `...` to match all remaining elements."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script implements an advanced version of the RMTPP model adapted for\n",
    "modeling multichannel neural waveform data. It uses a CNN encoder-decoder\n",
    "architecture to handle the high-dimensional waveform \"marks\".\n",
    "\"\"\"\n",
    "\n",
    "import functools\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import seaborn as sns\n",
    "from flax import nnx\n",
    "from jax import random\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration\n",
    "# ==============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration object for the RMTPP Waveform model and training.\"\"\"\n",
    "\n",
    "    # Data Simulation\n",
    "    n_sequences: int = 300\n",
    "    n_marks: int = 5  # Number of distinct waveform templates\n",
    "    max_seq_len: int = 50\n",
    "    n_channels: int = 8  # Number of channels on the linear probe\n",
    "    n_samples: int = 40  # Number of time samples per waveform clip\n",
    "\n",
    "    # Model Architecture\n",
    "    hidden_size: int = 64  # Increased hidden size for more complex data\n",
    "    embedding_size: int = 32  # Size of the compressed waveform vector\n",
    "\n",
    "    # Training\n",
    "    learning_rate: float = 1e-3\n",
    "    epochs: int = 200\n",
    "    batch_size: int = 16\n",
    "    test_split_ratio: float = 0.2\n",
    "    seed: int = 42\n",
    "    output_dir: Path = Path(\"./results_waveform\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Data Simulation (Generating Waveforms)\n",
    "# ==============================================================================\n",
    "def simulate_waveform_data(\n",
    "    config: Config,\n",
    ") -> List[Dict[str, jnp.ndarray]]:\n",
    "    \"\"\"\n",
    "    Simulates sequences of multichannel waveform clips.\n",
    "\n",
    "    Each \"mark\" is now a (n_channels, n_samples) array representing a spike\n",
    "    waveform as recorded on a linear probe.\n",
    "\n",
    "    Args:\n",
    "        config: The configuration object.\n",
    "\n",
    "    Returns:\n",
    "        A list of sequences, each a dictionary containing event times,\n",
    "        waveform clips, and a padding mask.\n",
    "    \"\"\"\n",
    "    key = random.PRNGKey(config.seed)\n",
    "    sequences = []\n",
    "\n",
    "    # Generate N waveform templates\n",
    "    key, subkey = random.split(key)\n",
    "    templates = random.normal(\n",
    "        subkey, (config.n_marks, config.n_channels, config.n_samples)\n",
    "    )\n",
    "\n",
    "    for i in range(config.n_marks):\n",
    "        # Create some structure in the templates\n",
    "        peak_time = config.n_samples // 3 + i\n",
    "        peak_channel = i % config.n_channels\n",
    "        amp = 2.0 + (i / config.n_marks)\n",
    "        for c in range(config.n_channels):\n",
    "            # Make amplitude decay away from the peak channel\n",
    "            dist = jnp.abs(c - peak_channel)\n",
    "            channel_amp = amp * jnp.exp(-dist / 2.0)\n",
    "            t = jnp.linspace(-5, 5, config.n_samples)\n",
    "            # A simple Gabor-like wavelet\n",
    "            templates = templates.at[i, c].set(\n",
    "                channel_amp\n",
    "                * jnp.exp(-((t - (peak_time - t.mean())) ** 2))\n",
    "                * jnp.cos(1.5 * (t - (peak_time - t.mean())))\n",
    "            )\n",
    "\n",
    "    for _ in range(config.n_sequences):\n",
    "        key, subkey_len, subkey_time, subkey_mark, subkey_noise = random.split(key, 5)\n",
    "        seq_len = random.randint(subkey_len, (), minval=20, maxval=config.max_seq_len)\n",
    "\n",
    "        times = np.zeros(config.max_seq_len, dtype=np.float32)\n",
    "        # The marks are now high-dimensional arrays\n",
    "        marks = np.zeros(\n",
    "            (\n",
    "                config.max_seq_len,\n",
    "                config.n_channels,\n",
    "                config.n_samples,\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        last_mark_idx = 0\n",
    "        times[0] = random.exponential(subkey_time, ()) * 0.1\n",
    "        marks[0] = (\n",
    "            templates[last_mark_idx]\n",
    "            + random.normal(subkey_noise, templates[0].shape) * 0.1\n",
    "        )\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            key, subkey_time, subkey_mark, subkey_noise = random.split(key, 4)\n",
    "\n",
    "            # Transition probability for next waveform type\n",
    "            probs = jnp.ones(config.n_marks) * 0.2\n",
    "            probs = probs.at[last_mark_idx].set(0.8)\n",
    "            next_mark_idx = random.choice(subkey_mark, config.n_marks, p=probs)\n",
    "\n",
    "            inter_event_time = random.exponential(subkey_time, ()) * (\n",
    "                1.0 + 0.5 * (next_mark_idx / config.n_marks)\n",
    "            )\n",
    "            times[i] = times[i - 1] + inter_event_time\n",
    "            marks[i] = (\n",
    "                templates[next_mark_idx]\n",
    "                + random.normal(subkey_noise, templates[0].shape) * 0.1\n",
    "            )\n",
    "            last_mark_idx = next_mark_idx\n",
    "\n",
    "        mask = np.arange(config.max_seq_len) < seq_len\n",
    "        sequences.append(\n",
    "            {\n",
    "                \"times\": jnp.asarray(times),\n",
    "                \"marks\": jnp.asarray(marks),\n",
    "                \"mask\": jnp.asarray(mask),\n",
    "            }\n",
    "        )\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Waveform Encoder and Decoder Modules\n",
    "# ==============================================================================\n",
    "class WaveformEncoder(nnx.Module):\n",
    "    \"\"\"A CNN to encode a waveform clip into a feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size: int, n_channels: int, *, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(\n",
    "            in_features=n_channels,\n",
    "            out_features=16,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"SAME\",\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.conv2 = nnx.Conv(\n",
    "            in_features=16,\n",
    "            out_features=32,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"SAME\",\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.dense = nnx.Linear(\n",
    "            in_features=32 * n_channels, out_features=embedding_size, rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Waveform clip with shape (batch, channels, samples).\n",
    "        \"\"\"\n",
    "        # Flax Conv expects channels-last format\n",
    "        x = x.transpose((0, 2, 1))\n",
    "        x = nnx.relu(self.conv1(x))\n",
    "        x = nnx.avg_pool(x, window_shape=(2,), strides=(2,))\n",
    "        x = nnx.relu(self.conv2(x))\n",
    "        x = nnx.avg_pool(x, window_shape=(2,), strides=(2,))\n",
    "        # Global average pooling over the time dimension\n",
    "        x = jnp.mean(x, axis=1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WaveformDecoder(nnx.Module):\n",
    "    \"\"\"A Transposed CNN to decode a feature vector into a waveform clip.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        n_channels: int,\n",
    "        n_samples: int,\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.n_channels = n_channels\n",
    "        self.n_samples = n_samples\n",
    "        self.dense = nnx.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=32 * (n_samples // 4) * n_channels,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.deconv1 = nnx.ConvTranspose(\n",
    "            in_features=32,\n",
    "            out_features=16,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"SAME\",\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.deconv2 = nnx.ConvTranspose(\n",
    "            in_features=16,\n",
    "            out_features=n_channels,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"SAME\",\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, h: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: Hidden state vector of shape (batch, hidden_size).\n",
    "        \"\"\"\n",
    "        x = self.dense(h)\n",
    "        # Reshape to an image-like format for deconvolution\n",
    "        x = x.reshape(\n",
    "            (\n",
    "                h.shape[0],\n",
    "                self.n_samples // 4,\n",
    "                self.n_channels,\n",
    "                32,\n",
    "            )\n",
    "        )\n",
    "        x = nnx.relu(self.deconv1(x))\n",
    "        x = self.deconv2(x)\n",
    "        # Transpose back to (batch, channels, samples)\n",
    "        x = x.transpose((0, 2, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Main RMTPP Model for Waveforms\n",
    "# ==============================================================================\n",
    "class RMTPP_Waveform(nnx.Module):\n",
    "    \"\"\"RMTPP model adapted for waveform data.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, *, rngs: nnx.Rngs):\n",
    "        self.encoder = WaveformEncoder(\n",
    "            embedding_size=config.embedding_size,\n",
    "            n_channels=config.n_channels,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.time_proj = nnx.Linear(\n",
    "            in_features=1, out_features=config.hidden_size, rngs=rngs\n",
    "        )\n",
    "        self.gru_cell = nnx.GRUCell(\n",
    "            in_features=config.embedding_size,\n",
    "            hidden_features=config.hidden_size,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.decoder = WaveformDecoder(\n",
    "            hidden_size=config.hidden_size,\n",
    "            n_channels=config.n_channels,\n",
    "            n_samples=config.n_samples,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        # Output layers for time prediction\n",
    "        self.v_proj = nnx.Linear(config.hidden_size, 1, use_bias=False, rngs=rngs)\n",
    "        self.w_proj = nnx.Linear(config.hidden_size, 1, use_bias=False, rngs=rngs)\n",
    "        self.b_proj = nnx.Linear(config.hidden_size, 1, use_bias=True, rngs=rngs)\n",
    "\n",
    "    def __call__(\n",
    "        self, times: jnp.ndarray, marks: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the model.\n",
    "        Now returns the predicted waveform instead of logits.\n",
    "        \"\"\"\n",
    "        inter_event_times = jnp.diff(times, prepend=0)\n",
    "        inter_event_times = jnp.expand_dims(inter_event_times, -1)\n",
    "\n",
    "        def rnn_step(h_prev, inputs):\n",
    "            mark_j, time_j = inputs\n",
    "            # Encode the waveform clip to get a feature vector\n",
    "            mark_emb = self.encoder(mark_j)\n",
    "            time_emb = self.time_proj(time_j)\n",
    "            # Input to the GRU is the mark embedding\n",
    "            # Note: A more complex model could combine these differently\n",
    "            h_j = self.gru_cell(h_prev, mark_emb + time_emb)\n",
    "            return h_j, h_j\n",
    "\n",
    "        batch_size, seq_len, _, _ = marks.shape\n",
    "        h0 = self.gru_cell.initialize_carry(\n",
    "            (batch_size, self.gru_cell.in_features), rngs=self.rngs\n",
    "        )\n",
    "\n",
    "        # We need to scan over marks and times together\n",
    "        marks_scannable = jnp.transpose(marks, (1, 0, 2, 3))\n",
    "        inter_event_times_scannable = jnp.transpose(inter_event_times, (1, 0, 2))\n",
    "\n",
    "        scan_fn = nnx.scan(rnn_step)\n",
    "        _, h_sequence = scan_fn(h0, (marks_scannable, inter_event_times_scannable))\n",
    "        h_sequence = h_sequence.transpose((1, 0, 2))\n",
    "\n",
    "        # Decode the hidden states to predict the next waveform\n",
    "        predicted_waveforms = self.decoder(h_sequence)\n",
    "\n",
    "        v_unconstrained = self.v_proj(h_sequence)\n",
    "        b_unconstrained = self.b_proj(h_sequence)\n",
    "        v = jax.nn.tanh(v_unconstrained) * 5.0\n",
    "        b = jax.nn.tanh(b_unconstrained) * 5.0\n",
    "        w = jax.nn.softplus(self.w_proj(h_sequence)) + 1e-6\n",
    "\n",
    "        return predicted_waveforms, v, w, b\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Training and Evaluation Logic\n",
    "# ==============================================================================\n",
    "def compute_loss(model: RMTPP_Waveform, batch: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "    \"\"\"Computes the combined loss for a batch of sequences.\"\"\"\n",
    "    times, marks, mask = batch[\"times\"], batch[\"marks\"], batch[\"mask\"]\n",
    "    is_last_event = jnp.cumsum(mask, axis=1) == jnp.sum(mask, axis=1, keepdims=True)\n",
    "    prediction_mask = mask & ~is_last_event\n",
    "\n",
    "    # Target for time is the next inter-event time\n",
    "    targets_times = jnp.roll(times, -1, axis=1)\n",
    "    inter_event_times_target = targets_times - times\n",
    "\n",
    "    # Target for mark is the next waveform in the sequence\n",
    "    targets_marks = jnp.roll(marks, -1, axis=1)\n",
    "\n",
    "    predicted_waveforms, v, w, b = model(times, marks)\n",
    "    v, w, b = v.squeeze(-1), w.squeeze(-1), b.squeeze(-1)\n",
    "\n",
    "    # 1. Mark Loss (Mean Squared Error for waveforms)\n",
    "    mark_loss = optax.squared_error(predicted_waveforms, targets_marks)\n",
    "    # Average over channel and sample dimensions\n",
    "    mark_loss = jnp.mean(mark_loss, axis=(-2, -1))\n",
    "    mark_loss = jnp.sum(mark_loss * prediction_mask) / jnp.sum(prediction_mask)\n",
    "\n",
    "    # 2. Time Loss (Negative Log-Likelihood)\n",
    "    log_lambda = v + w * inter_event_times_target + b\n",
    "    integral_lambda = (1 / w) * (\n",
    "        jnp.exp(v + w * inter_event_times_target + b) - jnp.exp(v + b)\n",
    "    )\n",
    "    time_loss = -log_lambda + integral_lambda\n",
    "    time_loss = jnp.sum(time_loss * prediction_mask) / jnp.sum(prediction_mask)\n",
    "\n",
    "    return mark_loss + time_loss\n",
    "\n",
    "\n",
    "# --- Training and Prediction functions are largely unchanged ---\n",
    "@functools.partial(nnx.jit, static_argnums=(0, 1))\n",
    "def train_step(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    tx: optax.GradientTransformation,\n",
    "    optimizer_state: optax.OptState,\n",
    "    params: nnx.State,\n",
    "    batch: Dict[str, jnp.ndarray],\n",
    ") -> Tuple[float, optax.OptState, nnx.State]:\n",
    "    \"\"\"Performs a single training step.\"\"\"\n",
    "\n",
    "    def loss_fn(current_params: nnx.State) -> float:\n",
    "        model = nnx.merge(graphdef, current_params)\n",
    "        return compute_loss(model, batch)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(params)\n",
    "    updates, optimizer_state = tx.update(grads, optimizer_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, optimizer_state, params\n",
    "\n",
    "\n",
    "@functools.partial(nnx.jit, static_argnums=(0,))\n",
    "def predict_next_event(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    params: nnx.State,\n",
    "    history_times: jnp.ndarray,\n",
    "    history_marks: jnp.ndarray,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Predicts the time and waveform of the next event.\"\"\"\n",
    "    model = nnx.merge(graphdef, params)\n",
    "    times_batch = jnp.expand_dims(history_times, 0)\n",
    "    marks_batch = jnp.expand_dims(history_marks, 0)\n",
    "    predicted_waveforms, v, w, b = model(times_batch, marks_batch)\n",
    "    last_v, last_w, last_b = v[0, -1, 0], w[0, -1, 0], b[0, -1, 0]\n",
    "\n",
    "    # Prediction for the waveform is the decoder output at the last step\n",
    "    predicted_waveform = predicted_waveforms[0, -1]\n",
    "\n",
    "    # Time prediction logic is unchanged\n",
    "    A = last_v + last_b\n",
    "    predicted_inter_event_time = (1 / last_w) * jnp.log(\n",
    "        1 + (last_w * jnp.log(2.0)) / jnp.exp(A)\n",
    "    )\n",
    "    predicted_time = history_times[-1] + predicted_inter_event_time\n",
    "    return predicted_time, predicted_waveform\n",
    "\n",
    "\n",
    "# --- Main loop and evaluation plotting functions need updating ---\n",
    "def train_model(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    tx: optax.GradientTransformation,\n",
    "    params: nnx.State,\n",
    "    sequences: List[Dict[str, jnp.ndarray]],\n",
    "    config: Config,\n",
    ") -> Tuple[nnx.State, List[float]]:\n",
    "    \"\"\"Handles the main training loop.\"\"\"\n",
    "    print(\"2. Starting model training...\")\n",
    "    optimizer_state = tx.init(params)\n",
    "    losses = []\n",
    "    n_batches = len(sequences) // config.batch_size\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_loss = 0.0\n",
    "        np.random.shuffle(sequences)\n",
    "        for i in range(n_batches):\n",
    "            batch_list = sequences[i * config.batch_size : (i + 1) * config.batch_size]\n",
    "            batch = {\n",
    "                \"times\": jnp.stack([s[\"times\"] for s in batch_list]),\n",
    "                \"marks\": jnp.stack([s[\"marks\"] for s in batch_list]),\n",
    "                \"mask\": jnp.stack([s[\"mask\"] for s in batch_list]),\n",
    "            }\n",
    "            loss_val, optimizer_state, params = train_step(\n",
    "                graphdef, tx, optimizer_state, params, batch\n",
    "            )\n",
    "            epoch_loss += loss_val\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config.epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
    "    print(\"3. Training complete.\")\n",
    "    return params, losses\n",
    "\n",
    "\n",
    "def evaluate_and_plot(\n",
    "    graphdef: nnx.GraphDef,\n",
    "    params: nnx.State,\n",
    "    test_sequences: List[Dict[str, jnp.ndarray]],\n",
    "    losses: List[float],\n",
    "    config: Config,\n",
    "):\n",
    "    \"\"\"Runs evaluation and generates result plots for waveform data.\"\"\"\n",
    "    # Plot Training Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Combined Loss (Time NLL + Waveform MSE)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(config.output_dir / \"training_loss.png\")\n",
    "    print(f\"\\n   - Saved training loss plot to '{config.output_dir}/'\")\n",
    "    plt.close()\n",
    "\n",
    "    # Gather Predictions\n",
    "    print(\"\\n4. Gathering predictions from the test set...\")\n",
    "    all_pred_times, all_actual_times = [], []\n",
    "    first_pred_waveform, first_actual_waveform = None, None\n",
    "\n",
    "    for i, seq in enumerate(test_sequences):\n",
    "        actual_seq_len = int(jnp.sum(seq[\"mask\"]))\n",
    "        if actual_seq_len < 10:\n",
    "            continue\n",
    "        history_len = actual_seq_len // 2\n",
    "        actual_event_index = history_len\n",
    "        history_times = seq[\"times\"][:history_len]\n",
    "        history_marks = seq[\"marks\"][:history_len]\n",
    "\n",
    "        pred_time, pred_waveform = predict_next_event(\n",
    "            graphdef, params, history_times, history_marks\n",
    "        )\n",
    "\n",
    "        actual_time = seq[\"times\"][actual_event_index]\n",
    "        actual_waveform = seq[\"marks\"][actual_event_index]\n",
    "\n",
    "        all_pred_times.append(pred_time)\n",
    "        all_actual_times.append(actual_time)\n",
    "        if i == 0:\n",
    "            first_pred_waveform = pred_waveform\n",
    "            first_actual_waveform = actual_waveform\n",
    "\n",
    "    # --- Calculate and Report Metrics ---\n",
    "    print(\"\\n5. Calculating aggregate test metrics...\")\n",
    "    time_errors = np.array(all_pred_times) - np.array(all_actual_times)\n",
    "    mae = np.mean(np.abs(time_errors))\n",
    "    rmse = np.sqrt(np.mean(np.square(time_errors)))\n",
    "    print(f\"   - Time Prediction MAE:    {mae:.4f}\")\n",
    "    print(f\"   - Time Prediction RMSE:   {rmse:.4f}\")\n",
    "\n",
    "    # --- Generate and Save Plots ---\n",
    "    print(\"\\n6. Generating evaluation plots...\")\n",
    "    plot_time_error_distribution(time_errors, config)\n",
    "    plot_waveform_comparison(first_actual_waveform, first_pred_waveform, config)\n",
    "\n",
    "\n",
    "def plot_time_error_distribution(time_errors: np.ndarray, config: Config):\n",
    "    \"\"\"Plots the distribution of time prediction errors.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(time_errors, kde=True)\n",
    "    plt.title(\"Distribution of Time Prediction Errors (Predicted - Actual)\")\n",
    "    plt.xlabel(\"Time Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(config.output_dir / \"time_error_distribution.png\")\n",
    "    print(\"   - Saved time error distribution plot.\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_waveform_comparison(\n",
    "    actual_waveform: jnp.ndarray,\n",
    "    pred_waveform: jnp.ndarray,\n",
    "    config: Config,\n",
    "):\n",
    "    \"\"\"Plots the actual vs. predicted waveform for one example.\"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        config.n_channels,\n",
    "        1,\n",
    "        figsize=(10, 2 * config.n_channels),\n",
    "        sharex=True,\n",
    "    )\n",
    "    fig.suptitle(\"Waveform Prediction vs. Actual\", fontsize=16)\n",
    "    time_axis = np.arange(config.n_samples)\n",
    "\n",
    "    for i in range(config.n_channels):\n",
    "        ax = axes[i]\n",
    "        ax.plot(\n",
    "            time_axis,\n",
    "            actual_waveform[i, :],\n",
    "            \"b-\",\n",
    "            label=\"Actual Waveform\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            time_axis,\n",
    "            pred_waveform[i, :],\n",
    "            \"r--\",\n",
    "            label=\"Predicted Waveform\",\n",
    "        )\n",
    "        ax.set_ylabel(f\"Channel {i}\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    axes[-1].set_xlabel(\"Time Samples\")\n",
    "    handles, labels = axes[-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper right\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(config.output_dir / \"waveform_comparison.png\")\n",
    "    print(\"   - Saved waveform comparison plot.\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run simulation, training, and plotting.\"\"\"\n",
    "    config = Config()\n",
    "    config.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print(\"1. Simulating waveform data...\")\n",
    "    sequences = simulate_waveform_data(config)\n",
    "    np.random.shuffle(sequences)\n",
    "    test_size = int(len(sequences) * config.test_split_ratio)\n",
    "    train_sequences, test_sequences = sequences[test_size:], sequences[:test_size]\n",
    "    model = RMTPP_Waveform(config=config, rngs=nnx.Rngs(config.seed))\n",
    "    tx = optax.chain(optax.clip(1.0), optax.adam(config.learning_rate))\n",
    "    graphdef, params = nnx.split(model, nnx.Param)\n",
    "    trained_params, losses = train_model(graphdef, tx, params, train_sequences, config)\n",
    "    evaluate_and_plot(graphdef, trained_params, test_sequences, losses, config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152beda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_local_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
