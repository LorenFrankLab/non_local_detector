{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59faac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0200] loss_lambda=-2.6851  loss_flow=11.2947\n",
      "[0400] loss_lambda=-2.6858  loss_flow=11.1851\n",
      "[0600] loss_lambda=-2.6890  loss_flow=11.0938\n",
      "[0800] loss_lambda=-2.6888  loss_flow=11.0561\n",
      "LL shape: (250, 400)\n"
     ]
    }
   ],
   "source": [
    "# flow_marked_pp_minimal.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax import struct\n",
    "\n",
    "# -----------------------------\n",
    "# Small utilities\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def searchsorted_right(edges: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.searchsorted(edges, x, side=\"right\")\n",
    "\n",
    "\n",
    "def bin_spike_times_to_bins(\n",
    "    spike_times: jnp.ndarray, time_edges: jnp.ndarray\n",
    ") -> jnp.ndarray:\n",
    "    idx = searchsorted_right(time_edges, spike_times) - 1\n",
    "    idx = jnp.clip(idx, 0, time_edges.shape[0] - 2)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def interp_position_at_times(\n",
    "    position_time: jnp.ndarray, position: jnp.ndarray, query_times: jnp.ndarray\n",
    ") -> jnp.ndarray:\n",
    "    # Linear interpolation on the time axis\n",
    "    idx_r = searchsorted_right(position_time, query_times)\n",
    "    idx_l = jnp.clip(idx_r - 1, 0, position_time.shape[0] - 1)\n",
    "    idx_r = jnp.clip(idx_r, 0, position_time.shape[0] - 1)\n",
    "    t0, t1 = position_time[idx_l], position_time[idx_r]\n",
    "    w = jnp.where(t1 > t0, (query_times - t0) / (t1 - t0 + 1e-9), 0.0)\n",
    "    p0, p1 = position[idx_l], position[idx_r]\n",
    "    return (1 - w)[:, None] * p0 + w[:, None] * p1  # (N,P)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Intensity network λ0(x)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "class LambdaNet(nn.Module):\n",
    "    \"\"\"Position-only baseline rate λ0(x). Output > 0 via softplus.\"\"\"\n",
    "\n",
    "    hidden: int = 128\n",
    "    depth: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        h = x\n",
    "        for _ in range(self.depth):\n",
    "            h = nn.silu(nn.Dense(self.hidden)(h))\n",
    "        return nn.softplus(nn.Dense(1)(h)).squeeze(-1) + 1e-6  # (N,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Conditional RealNVP-style flow for marks: m -> z\n",
    "# (custom, tiny; no external flow libs)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "class CouplingNN(nn.Module):\n",
    "    \"\"\"MLP that outputs shift and log_scale for the transformed dims.\"\"\"\n",
    "\n",
    "    out_dim: int\n",
    "    hidden: int = 128\n",
    "    depth: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, masked: jnp.ndarray, ctx: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        # masked: (N, d_mask); ctx: (N, C)\n",
    "        h = jnp.concatenate([masked, ctx], axis=-1)\n",
    "        for _ in range(self.depth):\n",
    "            h = nn.gelu(nn.Dense(self.hidden)(h))\n",
    "        raw = nn.Dense(self.out_dim)(h)  # 2 * d_trans\n",
    "        shift, log_scale = jnp.split(raw, 2, axis=-1)\n",
    "        log_scale = jnp.clip(log_scale, -5.0, 5.0)  # stabilize\n",
    "        return shift, log_scale\n",
    "\n",
    "\n",
    "class MarkFlow(nn.Module):\n",
    "    \"\"\"Conditional flow: log p(m | x) via coupling layers conditioned on x.\"\"\"\n",
    "\n",
    "    mark_dim: int\n",
    "    ctx_dim: int = 32\n",
    "    n_layers: int = 6\n",
    "    hidden: int = 128\n",
    "\n",
    "    def setup(self):\n",
    "        # Build alternating masks and their FIXED integer indices up front\n",
    "        masks = []\n",
    "        idx_m_list = []\n",
    "        idx_t_list = []\n",
    "        for i in range(self.n_layers):\n",
    "            mask = np.zeros(self.mark_dim, dtype=bool)\n",
    "            mask[i % 2 :: 2] = True\n",
    "            masks.append(mask)\n",
    "            idx_m_list.append(np.where(mask)[0].astype(np.int32))  # conditioned dims\n",
    "            idx_t_list.append(np.where(~mask)[0].astype(np.int32))  # transformed dims\n",
    "        self.masks = tuple(masks)  # optional (for debugging)\n",
    "        self.idx_m = tuple(idx_m_list)  # tuple of np.int32 arrays\n",
    "        self.idx_t = tuple(idx_t_list)\n",
    "\n",
    "        # Context net (from position x)\n",
    "        self.ctx_net = nn.Sequential([nn.Dense(self.ctx_dim), nn.tanh])\n",
    "\n",
    "        # One conditioner per layer; output = (shift, log_scale) for transformed dims\n",
    "        self.conds = tuple(\n",
    "            CouplingNN(out_dim=2 * len(self.idx_t[i]), hidden=self.hidden)\n",
    "            for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "    def __call__(self, m: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Return log p(m | x) using base N(0, I) and affine coupling transforms.\n",
    "        m: (N, D), x: (N, P)\n",
    "        \"\"\"\n",
    "        ctx = self.ctx_net(x)  # (N, C)\n",
    "        z = m\n",
    "        sum_log_det = jnp.zeros((m.shape[0],))\n",
    "        for idx_m, idx_t, cond in zip(self.idx_m, self.idx_t, self.conds):\n",
    "            # Use PRECOMPUTED integer indices (static) — no jnp.where in jit\n",
    "            z_m = z[:, idx_m]  # (N, d_m)\n",
    "            z_t = z[:, idx_t]  # (N, d_t)\n",
    "\n",
    "            shift, log_scale = cond(z_m, ctx)  # (N, d_t) each\n",
    "            log_scale = jnp.clip(log_scale, -5.0, 5.0)\n",
    "            z_t_new = (z_t - shift) * jnp.exp(-log_scale)\n",
    "            sum_log_det = sum_log_det - jnp.sum(log_scale, axis=-1)\n",
    "\n",
    "            z = z.at[:, idx_t].set(z_t_new)\n",
    "\n",
    "        D = self.mark_dim\n",
    "        log_pz = -0.5 * (jnp.sum(z * z, axis=-1) + D * jnp.log(2.0 * jnp.pi))\n",
    "        return log_pz + sum_log_det\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config & state\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    position_dim: int = 2\n",
    "    mark_dim: int = 8\n",
    "    hidden: int = 128\n",
    "    depth_lambda: int = 2\n",
    "    ctx_dim: int = 32\n",
    "    n_flow_layers: int = 6\n",
    "    lr_lambda: float = 1e-3\n",
    "    lr_flow: float = 1e-3\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class ModelState:\n",
    "    # pytree leaves (OK inside jit)\n",
    "    params_lambda: dict\n",
    "    params_flow: dict\n",
    "    opt_lambda: optax.OptState\n",
    "    opt_flow: optax.OptState\n",
    "\n",
    "    # static (non-pytree) fields\n",
    "    tx_lambda: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    tx_flow: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    lambda_net: LambdaNet = struct.field(pytree_node=False)\n",
    "    mark_flow: MarkFlow = struct.field(pytree_node=False)\n",
    "    cfg: ModelConfig = struct.field(pytree_node=False)\n",
    "\n",
    "\n",
    "def init_model(rng: jax.random.KeyArray, cfg: ModelConfig) -> ModelState:\n",
    "    rng_lam, rng_flow = jax.random.split(rng, 2)\n",
    "    lambda_net = LambdaNet(hidden=cfg.hidden, depth=cfg.depth_lambda)\n",
    "    mark_flow = MarkFlow(\n",
    "        mark_dim=cfg.mark_dim,\n",
    "        ctx_dim=cfg.ctx_dim,\n",
    "        n_layers=cfg.n_flow_layers,\n",
    "        hidden=cfg.hidden,\n",
    "    )\n",
    "\n",
    "    # Dummy shapes to initialize params\n",
    "    x_dummy = jnp.zeros((1, cfg.position_dim))\n",
    "    m_dummy = jnp.zeros((1, cfg.mark_dim))\n",
    "\n",
    "    params_lambda = lambda_net.init(rng_lam, x_dummy)\n",
    "    params_flow = mark_flow.init(rng_flow, m_dummy, x_dummy)\n",
    "\n",
    "    tx_lambda = optax.adamw(cfg.lr_lambda)\n",
    "    tx_flow = optax.adamw(cfg.lr_flow)\n",
    "    opt_lambda = tx_lambda.init(params_lambda)\n",
    "    opt_flow = tx_flow.init(params_flow)\n",
    "\n",
    "    return ModelState(\n",
    "        params_lambda=params_lambda,\n",
    "        params_flow=params_flow,\n",
    "        opt_lambda=opt_lambda,\n",
    "        opt_flow=opt_flow,\n",
    "        tx_lambda=tx_lambda,\n",
    "        tx_flow=tx_flow,\n",
    "        lambda_net=lambda_net,\n",
    "        mark_flow=mark_flow,\n",
    "        cfg=cfg,\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training: marked Poisson likelihood (no history => integral = Δt * λ0)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss_step(\n",
    "    state: ModelState,\n",
    "    x_spk: jnp.ndarray,\n",
    "    m_spk: jnp.ndarray,\n",
    "    x_time: jnp.ndarray,\n",
    "    dt: float,\n",
    ") -> Tuple[ModelState, dict]:\n",
    "    \"\"\"\n",
    "    One step: minimize\n",
    "      - E_spikes [ log λ0(x) + log p_flow(m|x) ] + E_time [ λ0(x) * dt ]\n",
    "    \"\"\"\n",
    "\n",
    "    dt = jnp.asarray(dt)\n",
    "\n",
    "    # Lambda params update (appears in events + integral)\n",
    "    def lam_total_loss(params_lambda):\n",
    "        lam_spk = state.lambda_net.apply(params_lambda, x_spk)\n",
    "        lam_time = state.lambda_net.apply(params_lambda, x_time)\n",
    "        # events: -log λ\n",
    "        events = -jnp.log(lam_spk + 1e-12).mean()\n",
    "        # integral: + E_t[ λ * dt ]\n",
    "        integral = lam_time.mean() * dt\n",
    "        return events + integral\n",
    "\n",
    "    # Flow params update (only in mark terms)\n",
    "    def flow_loss(params_flow):\n",
    "        lp_mark = state.mark_flow.apply(params_flow, m_spk, x_spk)  # log p(m|x)\n",
    "        return -lp_mark.mean()\n",
    "\n",
    "    lam_value, lam_grads = jax.value_and_grad(lam_total_loss)(state.params_lambda)\n",
    "    flow_value, flow_grads = jax.value_and_grad(flow_loss)(state.params_flow)\n",
    "\n",
    "    upd_lam, opt_lam = state.tx_lambda.update(\n",
    "        lam_grads, state.opt_lambda, state.params_lambda\n",
    "    )\n",
    "    upd_flow, opt_flow = state.tx_flow.update(\n",
    "        flow_grads, state.opt_flow, state.params_flow\n",
    "    )\n",
    "\n",
    "    params_lambda = optax.apply_updates(state.params_lambda, upd_lam)\n",
    "    params_flow = optax.apply_updates(state.params_flow, upd_flow)\n",
    "\n",
    "    new_state = ModelState(\n",
    "        params_lambda=params_lambda,\n",
    "        params_flow=params_flow,\n",
    "        opt_lambda=opt_lam,\n",
    "        opt_flow=opt_flow,\n",
    "        tx_lambda=state.tx_lambda,\n",
    "        tx_flow=state.tx_flow,\n",
    "        lambda_net=state.lambda_net,\n",
    "        mark_flow=state.mark_flow,\n",
    "        cfg=state.cfg,\n",
    "    )\n",
    "    info = dict(loss_lambda=lam_value, loss_flow=flow_value)\n",
    "    return new_state, info\n",
    "\n",
    "\n",
    "def train(\n",
    "    rng: jax.random.KeyArray,\n",
    "    position_time: jnp.ndarray,  # (T_pos,)\n",
    "    position: jnp.ndarray,  # (T_pos, P)\n",
    "    spike_times: jnp.ndarray,  # (S,)\n",
    "    marks: jnp.ndarray,  # (S, M)\n",
    "    cfg: ModelConfig,\n",
    "    steps: int = 2000,\n",
    "    batch_spikes: int = 1024,\n",
    "    batch_time: int = 2048,\n",
    ") -> ModelState:\n",
    "    state = init_model(rng, cfg)\n",
    "    T = position_time.shape[0]\n",
    "    dt = float(jnp.mean(jnp.diff(position_time)))\n",
    "    rng_np = np.random.default_rng(0)\n",
    "\n",
    "    # Precompute spike positions\n",
    "    st = spike_times\n",
    "    x_spk_all = interp_position_at_times(position_time, position, st)\n",
    "\n",
    "    for step in range(steps):\n",
    "        # sample spikes\n",
    "        if st.shape[0] > 0:\n",
    "            sel_s = rng_np.choice(\n",
    "                st.shape[0],\n",
    "                size=min(batch_spikes, st.shape[0]),\n",
    "                replace=st.shape[0] < batch_spikes,\n",
    "            )\n",
    "            x_spk = x_spk_all[sel_s]\n",
    "            m_spk = marks[sel_s]\n",
    "        else:\n",
    "            x_spk = jnp.zeros((1, cfg.position_dim))\n",
    "            m_spk = jnp.zeros((1, cfg.mark_dim))\n",
    "\n",
    "        # sample time points for integral\n",
    "        sel_t = rng_np.choice(T, size=min(batch_time, T), replace=T < batch_time)\n",
    "        x_time = position[sel_t]\n",
    "\n",
    "        state, info = loss_step(state, x_spk, m_spk, x_time, dt)\n",
    "        if (step + 1) % 200 == 0:\n",
    "            print(\n",
    "                f\"[{step+1:04d}] loss_lambda={float(info['loss_lambda']):.4f}  loss_flow={float(info['loss_flow']):.4f}\"\n",
    "            )\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Non-local decoding (no history; add exp(h(t)) as a scalar multiplier if you like)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def nonlocal_log_likelihood(\n",
    "    state: ModelState,\n",
    "    time_edges: jnp.ndarray,  # (N+1,)\n",
    "    X_grid: jnp.ndarray,  # (B, P)\n",
    "    spike_times: jnp.ndarray,  # (S,)\n",
    "    marks: jnp.ndarray,  # (S, M)\n",
    "    block_size: int = 128,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Returns LL over (time bin, position bin): shape (N, B).\n",
    "    For each bin n and position x_b:\n",
    "        Sum_{spikes in n} [ log λ0(x_b) + log p_flow(m_i | x_b) ] - Δt_n * λ0(x_b)\n",
    "    \"\"\"\n",
    "    N = time_edges.shape[0] - 1\n",
    "    B = X_grid.shape[0]\n",
    "    dt = time_edges[1:] - time_edges[:-1]  # (N,)\n",
    "    out = jnp.zeros((N, B))\n",
    "\n",
    "    # Bin spikes\n",
    "    inb = jnp.logical_and(spike_times >= time_edges[0], spike_times < time_edges[-1])\n",
    "    st = spike_times[inb]\n",
    "    m = marks[inb]\n",
    "    idx = (\n",
    "        bin_spike_times_to_bins(st, time_edges)\n",
    "        if st.shape[0] > 0\n",
    "        else jnp.array([], dtype=jnp.int32)\n",
    "    )\n",
    "\n",
    "    # Integral term: - Δt * λ0(x_b)\n",
    "    for s in range(0, B, block_size):\n",
    "        xb = X_grid[s : s + block_size]\n",
    "        lam = state.lambda_net.apply(state.params_lambda, xb)  # (b,)\n",
    "        out = out.at[:, s : s + block_size].add(-dt[:, None] * lam[None, :])\n",
    "\n",
    "    # Spike terms\n",
    "    if st.shape[0] > 0:\n",
    "        for s in range(0, B, block_size):\n",
    "            xb = X_grid[s : s + block_size]  # (b,P)\n",
    "            # tile spikes across positions\n",
    "            S = m.shape[0]\n",
    "            b = xb.shape[0]\n",
    "            Xrep = jnp.repeat(xb[None, :, :], S, axis=0).reshape(S * b, -1)\n",
    "            Mrep = jnp.repeat(m[:, None, :], b, axis=1).reshape(S * b, -1)\n",
    "            # scores\n",
    "            lam_rep = state.lambda_net.apply(state.params_lambda, Xrep).reshape(\n",
    "                S, b\n",
    "            )  # (S,b)\n",
    "            lp_mark = state.mark_flow.apply(state.params_flow, Mrep, Xrep).reshape(S, b)\n",
    "            contrib = jnp.log(lam_rep + 1e-12) + lp_mark  # (S,b)\n",
    "            add_blk = jax.ops.segment_sum(contrib, idx, num_segments=N)  # (N,b)\n",
    "            out = out.at[:, s : s + block_size].add(add_blk)\n",
    "\n",
    "    return out  # (N,B)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage (toy, to check shapes)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "\n",
    "    # Fake positions on a 2D arena\n",
    "    T = 4000\n",
    "    P = 2\n",
    "    M = 8\n",
    "    t_pos = jnp.linspace(0.0, 100.0, T)\n",
    "    pos = jnp.stack([jnp.sin(0.05 * t_pos), jnp.cos(0.05 * t_pos)], axis=-1)  # (T,2)\n",
    "\n",
    "    # Fake spikes + marks\n",
    "    S = 6000\n",
    "    st = jnp.sort(jax.random.uniform(rng, (S,)) * t_pos[-1])\n",
    "    marks = jax.random.normal(rng, (S, M))\n",
    "\n",
    "    cfg = ModelConfig(position_dim=P, mark_dim=M, hidden=128, n_flow_layers=6)\n",
    "    state = train(rng, t_pos, pos, st, marks, cfg, steps=800)\n",
    "\n",
    "    # Non-local decode: 20 ms bins over a 5 s window; 20x20 grid\n",
    "    edges = jnp.linspace(10.0, 15.0, 251)\n",
    "    xs = jnp.linspace(-1.5, 1.5, 20)\n",
    "    grid = jnp.stack(jnp.meshgrid(xs, xs), axis=-1).reshape(-1, 2)\n",
    "    ll = nonlocal_log_likelihood(state, edges, grid, st, marks, block_size=64)\n",
    "    print(\"LL shape:\", ll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7acca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_local_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
